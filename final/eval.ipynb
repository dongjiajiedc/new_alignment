{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper import *\n",
    "from alignment import *\n",
    "from datasets.preprecossing import *\n",
    "from core import *\n",
    "from datasets.loading import *\n",
    "from datasets.hc_dataset import *\n",
    "from datasets.balance_dataset import *\n",
    "from utils.linkage import *\n",
    "from model.balancehc import balancehc\n",
    "\n",
    "from utils.poincare import *\n",
    "import scib\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scib\n",
    "\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from os.path import exists\n",
    "def get_count_data(adata,counts_location=None):\n",
    "    \n",
    "    data = adata.layers[counts_location].copy() if counts_location else adata.X.copy()\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data= data.toarray()\n",
    "    data_df = pd.DataFrame(data,index=adata.obs_names,columns=adata.var_names).transpose()\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def check_paths(output_folder,output_prefix=None):\n",
    "    # Create relative path\n",
    "    output_path = os.path.join(os.getcwd(), output_folder)\n",
    "\n",
    "    # Make sure that the folder exists\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(os.path.join(output_path, f\"{output_prefix}assigned_locations.csv\")):\n",
    "        print(\"\\033[91mWARNING\\033[0m: Running this will overwrite previous results, choose a new\"\n",
    "              \" 'output_folder' or 'output_prefix'\")\n",
    "\n",
    "    return output_path\n",
    "def remove_batch_effect(pseudo_bulk, bulk_adata, out_dir, project='',batch_effect=True):\n",
    "    \"\"\"\n",
    "    Remove batch effect between pseudo_bulk and input bulk data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pseudo_bulk : anndata.AnnData\n",
    "        An :class:`~anndata.AnnData` containing the pseudo expression.\n",
    "    bulk_adata : anndata.AnnData\n",
    "        An :class:`~anndata.AnnData` containing the input expression.\n",
    "    out_dir : string, optional\n",
    "        The path to save the output file.\n",
    "    project : string, optional\n",
    "        The prefix of output file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Returns the expression after removing batch effect.\n",
    "\n",
    "    \"\"\"\n",
    "    out_dir = check_paths(out_dir+'/batch_effect')\n",
    "    if batch_effect:\n",
    "        if exists(f'{out_dir}/{project}_batch_effected.txt'):\n",
    "            print(f'{out_dir}/{project}_batch_effected.txt already exists, skipping batch effect.')\n",
    "            bulk_data = pd.read_csv(f\"{out_dir}/{project}_batch_effected.txt\",sep='\\t').T\n",
    "        else:\n",
    "            \n",
    "            save=True\n",
    "            # check path, file will be stored in out_dir+'/batch_effect'\n",
    "            pseudo_bulk_df = get_count_data(pseudo_bulk)\n",
    "            input_bulk_df= get_count_data(bulk_adata)\n",
    "\n",
    "            bulk = pd.concat([pseudo_bulk_df,input_bulk_df], axis=1)\n",
    "\n",
    "            cells = np.append(pseudo_bulk.obs_names, bulk_adata.obs_names)\n",
    "            batch = np.append(np.ones((1,len(pseudo_bulk.obs_names))), np.ones((1,len(bulk_adata.obs_names)))+1)\n",
    "            if save:\n",
    "                bulk.to_csv(out_dir+f\"/{project}_before_batch_effected.txt\",sep='\\t')\n",
    "            meta = pd.DataFrame({\"batch\": batch,\"cells\":cells})\n",
    "            # get r script path\n",
    "            robjects.r.source('./combat.R')\n",
    "            pandas2ri.activate()\n",
    "            robjects.r.run_combat(bulk, meta, out_dir, project)\n",
    "            # stop auto trans from pandas to r dataframe\n",
    "            pandas2ri.deactivate()\n",
    "            # add layer\n",
    "            if exists(f'{out_dir}/{project}_batch_effected.txt'):\n",
    "                bulk_data = pd.read_csv(f\"{out_dir}/{project}_batch_effected.txt\",sep='\\t').T\n",
    "                print(bulk_data.shape)\n",
    "            else:\n",
    "                raise ValueError('The batch_effected data is not available')\n",
    "        bulk_data.clip(lower=0,inplace=True)\n",
    "        # print(pseudo_bulk)\n",
    "        # print(pseudo_bulk.obs_names)\n",
    "        pseudo_bulk.layers[\"batch_effected\"] = bulk_data.loc[pseudo_bulk.obs_names,:].values\n",
    "        bulk_adata.layers[\"batch_effected\"] = bulk_data.loc[bulk_adata.obs_names,:].values\n",
    "    else:\n",
    "        pseudo_bulk_df = get_count_data(pseudo_bulk)\n",
    "        input_bulk_df= get_count_data(bulk_adata)\n",
    "        bulk = pd.concat([pseudo_bulk_df,input_bulk_df], axis=1)\n",
    "        bulk.to_csv(out_dir+f\"/{project}_batch_effected.txt\",sep='\\t')\n",
    "\n",
    "    return pseudo_bulk,bulk_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_path1 = './datas/32/1/BRCA_GSE110686.h5ad'\n",
    "folder_path1 = './datas/32/1/'\n",
    "radius1 = 0\n",
    "c1 =0.1\n",
    "epoches1 =40\n",
    "cell_path2 = \"./datas/32/2/8000_sample.h5ad\" \n",
    "folder_path2 = \"./datas/32/2/\" \n",
    "radius2 = 0\n",
    "c2 =0.1\n",
    "epoches2 = 40\n",
    "contin = False\n",
    "resolution=0.5\n",
    "method='average'\n",
    "alignment=1\n",
    "n_pca=100\n",
    "meta_col = 'Celltype..major.lineage.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_sc.py -cp1 './datas/120/3/sample1_small.h5' -f1 \"./datas/120/3/\" -r1 25 -c1 0.1 -e1 10 -cp2 './datas/120/4/sample1_small.h5' -f2 \"./datas/120/4/\" -r2 25 -c2 0.1 -e2 10 --contin False --alignment 1 --resolution 1 --n_pca 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/orig.ident' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/nCount_RNA' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/nFeature_RNA' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/UMAP_1' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/UMAP_2' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype..malignancy.' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype..major.lineage.' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype..minor.lineage.' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Cluster' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Patient' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Source' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Gender' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Stage' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/_index' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:238: OldFormatWarning: Element '/obsm' was written without encoding metadata.\n",
      "  d[k] = read_elem(f[k])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/var/features' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/var/_index' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:238: OldFormatWarning: Element '/varm' was written without encoding metadata.\n",
      "  d[k] = read_elem(f[k])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:272: OldFormatWarning: Element '/raw/var' was written without encoding metadata.\n",
      "  raw[v] = read_elem(f[f\"raw/{v}\"])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:92: OldFormatWarning: Element '/raw/var/_index' was written without encoding metadata.\n",
      "  return {k: read_elem(v) for k, v in elem.items()}\n",
      "100%|██████████████████████████████████| 6035/6035 [00:00<00:00, 4983781.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell meta score for dataset1: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bm/69jvgsdj3rxch558kx9yp9y00000gn/T/ipykernel_40421/2407986756.py:12: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  adata1 = anndata.AnnData(adata1)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_core/anndata.py:121: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:208: RuntimeWarning: invalid value encountered in log\n",
      "  dispersion = np.log(dispersion)\n",
      "100%|██████████████████████████████████| 8000/8000 [00:00<00:00, 7752872.46it/s]\n"
     ]
    }
   ],
   "source": [
    "if (contin==False) or ((os.path.exists(folder_path1+'merge_cell_data.csv') and os.path.exists(folder_path1 + 'merge_cell_meta.csv')) == False):\n",
    "    loss1 = merge_by_radius(cell_path1,folder_path1,radius1,method,meta_col)\n",
    "    print(\"cell meta score for dataset1: {}\\n\".format(loss1))\n",
    "else:\n",
    "    print(\"dataset1 find files and skip merging\")\n",
    "\n",
    "\n",
    "adata1 = pd.read_csv(folder_path1+\"merge_cell_data.csv\")\n",
    "cell_meta = pd.read_csv(folder_path1+\"merge_cell_meta.csv\")\n",
    "cell_meta = cell_meta.set_index(cell_meta.columns[0])\n",
    "adata1 = adata1.set_index(adata1.columns[0])\n",
    "adata1 = anndata.AnnData(adata1)\n",
    "adata1.obs['celltype'] = cell_meta.values.reshape(-1)\n",
    "\n",
    "\n",
    "if(contin==False) or ((os.path.exists(folder_path2+'merge_cell_data.csv') and os.path.exists(folder_path2 + 'merge_cell_meta.csv')) == False):\n",
    "    loss2 = merge_by_radius(cell_path2,folder_path2,radius2,method,meta_col)\n",
    "    print(\"cell meta score for dataset2: {}\".format(loss2))\n",
    "else:\n",
    "    print(\"dataset2 find files and skip merging\")\n",
    "\n",
    "adata2 = pd.read_csv(folder_path2+\"merge_cell_data.csv\")\n",
    "cell_meta = pd.read_csv(folder_path2+\"merge_cell_meta.csv\")\n",
    "cell_meta = cell_meta.set_index(cell_meta.columns[0])\n",
    "adata2 = adata2.set_index(adata2.columns[0])\n",
    "adata2 = anndata.AnnData(adata2)\n",
    "adata2.obs['celltype'] = cell_meta.values.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_cluster(adata1,N_pcs=n_pca,resolution=resolution)\n",
    "preprocessing_cluster(adata2,N_pcs=n_pca,resolution=resolution)\n",
    "\n",
    "inter_gene = sort_data(adata1,adata2)\n",
    "\n",
    "tmp1 = calculate_cluster_centroid_for_genes(adata1,inter_gene,folder_path1)\n",
    "tmp2 = calculate_cluster_centroid_for_genes(adata2,inter_gene,folder_path2)\n",
    "\n",
    "ari = adjusted_rand_score(adata1.obs['celltype'].tolist(), adata1.obs['leiden'].tolist())\n",
    "print(\"ARI score for adata1: \", ari)\n",
    "\n",
    "ari = adjusted_rand_score(adata2.obs['celltype'].tolist(), adata2.obs['leiden'].tolist())\n",
    "print(\"ARI score for adata2: \", ari)\n",
    "\n",
    "meta_list1 = []\n",
    "clustername = adata1.obs['leiden'].unique().tolist()\n",
    "clustername = list(map(int, clustername))\n",
    "clustername.sort()\n",
    "for value in clustername:\n",
    "    indices = [i for i, x in enumerate(adata1.obs['leiden']) if x == str(value)]\n",
    "    t = [adata1.obs['celltype'].tolist()[index] for index in indices]\n",
    "    most_common_element = max(t, key=t.count)\n",
    "    meta_list1.append(most_common_element)\n",
    "np.save(folder_path1+'tree_merge.npy',meta_list1)\n",
    "\n",
    "    \n",
    "meta_list2 = []\n",
    "clustername = adata2.obs['leiden'].unique().tolist()\n",
    "clustername = list(map(int, clustername))\n",
    "clustername.sort()\n",
    "for value in clustername:\n",
    "    indices = [i for i, x in enumerate(adata2.obs['leiden']) if x == str(value)]\n",
    "    t = [adata2.obs['celltype'].tolist()[index] for index in indices]\n",
    "    most_common_element = max(t, key=t.count)\n",
    "    meta_list2.append(most_common_element)\n",
    "np.save(folder_path2+'tree_merge.npy',meta_list2)\n",
    "\n",
    "\n",
    "v1 = pd.read_csv(folder_path1+\"merge_labels.csv\")\n",
    "meta = pd.read_csv(folder_path1+\"merge_cell_meta.csv\")\n",
    "meta = meta.set_index(meta.columns[0])\n",
    "meta\n",
    "lisan = []\n",
    "julei = []\n",
    "for i in range(len(v1)):\n",
    "    lisan.append(meta.iloc[v1['label'][i]][0])\n",
    "    julei.append(adata1.obs['leiden'].iloc[v1['label'][i]][0])\n",
    "v1['first']=lisan\n",
    "v1['second']=julei\n",
    "v1.to_csv(folder_path1+'meta_result.csv',index=None)\n",
    "\n",
    "v1 = pd.read_csv(folder_path2+\"merge_labels.csv\")\n",
    "meta = pd.read_csv(folder_path2+\"merge_cell_meta.csv\")\n",
    "meta = meta.set_index(meta.columns[0])\n",
    "meta\n",
    "lisan = []\n",
    "julei = []\n",
    "for i in range(len(v1)):\n",
    "    lisan.append(meta.iloc[v1['label'][i]][0])\n",
    "    julei.append(adata2.obs['leiden'].iloc[v1['label'][i]][0])\n",
    "v1['first']=lisan\n",
    "v1['second']=julei\n",
    "v1.to_csv(folder_path2+'meta_result.csv',index=None)\n",
    "\n",
    "if(contin==False) or ((os.path.exists(folder_path1 + 'dataxy.npy') and os.path.exists(folder_path1+'data1link.npy') and os.path.exists(folder_path1+'dataname.npy')) == False):\n",
    "    get_Hyper_tree(folder_path1+'datas.data',1,tmp1.shape[1]+1,0,epoches1,save_path=folder_path1)\n",
    "else:\n",
    "    print(\"dataset1 find files and skip embedding\");\n",
    "\n",
    "if(contin==False) or ((os.path.exists(folder_path2 + 'dataxy.npy') and os.path.exists(folder_path2+'data1link.npy') and os.path.exists(folder_path1+'dataname.npy')) == False):\n",
    "    get_Hyper_tree(folder_path2+'datas.data',1,tmp2.shape[1]+1,0,epoches2,save_path=folder_path2)\n",
    "else:\n",
    "    print(\"dataset2 find files and skip embedding\")\n",
    "\n",
    "    \n",
    "nodes1,n1 = build_hyper_tree_from_folder(folder_path1)\n",
    "nodes2,n2 = build_hyper_tree_from_folder(folder_path2)\n",
    "\n",
    "merge_list1 = [];\n",
    "merge_list2 = [];\n",
    "nodes1[0] = search_tree(nodes1[0],c1,merge_list1)\n",
    "nodes2[0] = search_tree(nodes2[0],c2,merge_list2)\n",
    "\n",
    "for i in range(len(nodes1)):\n",
    "    if(int(nodes1[i].name)<len(meta_list1)):\n",
    "        nodes1[i].name= nodes1[i].name +'_'+ meta_list1[int(nodes1[i].name)];\n",
    "        \n",
    "for i in range(len(nodes2)):\n",
    "    if(int(nodes2[i].name)<len(meta_list2)):\n",
    "        nodes2[i].name= nodes2[i].name +'_'+ meta_list2[int(nodes2[i].name)];  \n",
    "        \n",
    "rate = 0;        \n",
    "if(alignment==1):\n",
    "    rate,anslist,ans = run_alignment(nodes1,nodes2,folder_path1,folder_path2,meta_list1,meta_list2);\n",
    "elif(alignment==2):\n",
    "    rate = run_alignment_linear(nodes1,nodes2);\n",
    "    \n",
    "rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anslist_dist = dict(ans)\n",
    "anslist_dist.keys()\n",
    "def search_lineage(now,path,anss):\n",
    "    path.append(now.name)\n",
    "    if(now.son==[]):\n",
    "        anss.append(path);\n",
    "        return\n",
    "    \n",
    "    for i in now.son:\n",
    "        search_lineage(i,path.copy(),anss);\n",
    "temp1 = []\n",
    "search_lineage(nodes1[0],[],temp1)\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route1 = []\n",
    "route2 = []\n",
    "\n",
    "for i in temp1:\n",
    "    r1 = []\n",
    "    r2 = []\n",
    "    for j in i:\n",
    "        if j in anslist_dist.keys():\n",
    "            r1.append(j)\n",
    "            r2.append(anslist_dist[j])\n",
    "    route1.append(r1)\n",
    "    route2.append(r2)\n",
    "route2,route1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata1.obs.index = [i+'_1' for i in adata1.obs.index]\n",
    "adata2.obs.index = [i+'_2' for i in adata2.obs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [];\n",
    "\n",
    "\n",
    "for i,j in zip(route1,route2):\n",
    "    try:\n",
    "        shutil.rmtree('./batch_effect/', ignore_errors=True)\n",
    "\n",
    "        cells1 = [ ]\n",
    "        cells2 = [ ]\n",
    "        for k,t in zip(i,j):\n",
    "            num1 = int(k.split('_')[0])\n",
    "            num2 = int(t.split('_')[0])\n",
    "            if(num1 < (n1+1)/2 and num2 < (n2+1)/2):\n",
    "                # cells1.append(num1)\n",
    "                # cells2.append(num2)\n",
    "                cells1.append(str(num1))\n",
    "                cells2.append(str(num2))\n",
    "        if(cells1==[] or cells2 ==[]):\n",
    "            continue;\n",
    "        \n",
    "        sub_adata1 = adata1[adata1.obs['leiden'].isin(cells1),inter_gene].copy();\n",
    "        sub_adata2 = adata2[adata2.obs['leiden'].isin(cells2),inter_gene].copy();\n",
    "\n",
    "        adata1_after,adata2_after = remove_batch_effect(sub_adata1.copy(),sub_adata2.copy(),'./')\n",
    "        \n",
    "        sc.pp.neighbors(sub_adata1,use_rep='X')\n",
    "        sc.tl.diffmap(sub_adata1)\n",
    "        sub_adata1.uns['iroot'] = 0\n",
    "        sc.tl.dpt(sub_adata1)\n",
    "        \n",
    "        \n",
    "        sc.pp.neighbors(sub_adata2,use_rep='X')\n",
    "        sc.tl.diffmap(sub_adata2)\n",
    "        sub_adata2.uns['iroot'] = 0\n",
    "        sc.tl.dpt(sub_adata2)\n",
    "        \n",
    "        \n",
    "        adata1_after.obsm['batch_effected'] = adata1_after.layers['batch_effected']\n",
    "        adata2_after.obsm['batch_effected'] = adata2_after.layers['batch_effected']\n",
    "        \n",
    "        sc.pp.neighbors(adata1_after,use_rep='batch_effected')\n",
    "        sc.tl.diffmap(adata1_after)\n",
    "        adata1_after.uns['iroot'] = 0\n",
    "        sc.tl.dpt(adata1_after)\n",
    "\n",
    "        sc.pp.neighbors(adata2_after,use_rep='batch_effected')\n",
    "        sc.tl.diffmap(adata2_after)\n",
    "        adata2_after.uns['iroot'] = 0\n",
    "        sc.tl.dpt(adata2_after)\n",
    "        \n",
    "        score.append( scib.me.trajectory_conservation(sub_adata1, adata1_after, label_key=\"celltype\"))\n",
    "        score.append( scib.me.trajectory_conservation(sub_adata2, adata2_after, label_key=\"celltype\"))\n",
    "        shutil.rmtree('./batch_effect/', ignore_errors=True)\n",
    "    except:\n",
    "        print(cells1,cells2)\n",
    "        pass;\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(score).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7413529924699612 啥也没有\n",
    "0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7413529924699612\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7554990228993519 houmian"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
