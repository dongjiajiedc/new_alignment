{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper import *\n",
    "from alignment import *\n",
    "from datasets.preprecossing import *\n",
    "from core import *\n",
    "from datasets.loading import *\n",
    "from datasets.hc_dataset import *\n",
    "from datasets.balance_dataset import *\n",
    "from utils.linkage import *\n",
    "from model.balancehc import balancehc\n",
    "\n",
    "from utils.poincare import *\n",
    "import scib\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(46055) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(46056) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scib\n",
    "\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from os.path import exists\n",
    "def get_count_data(adata,counts_location=None):\n",
    "    \n",
    "    data = adata.layers[counts_location].copy() if counts_location else adata.X.copy()\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data= data.toarray()\n",
    "    data_df = pd.DataFrame(data,index=adata.obs_names,columns=adata.var_names).transpose()\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def check_paths(output_folder,output_prefix=None):\n",
    "    # Create relative path\n",
    "    output_path = os.path.join(os.getcwd(), output_folder)\n",
    "\n",
    "    # Make sure that the folder exists\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(os.path.join(output_path, f\"{output_prefix}assigned_locations.csv\")):\n",
    "        print(\"\\033[91mWARNING\\033[0m: Running this will overwrite previous results, choose a new\"\n",
    "              \" 'output_folder' or 'output_prefix'\")\n",
    "\n",
    "    return output_path\n",
    "def remove_batch_effect(pseudo_bulk, bulk_adata, out_dir, project='',batch_effect=True):\n",
    "    \"\"\"\n",
    "    Remove batch effect between pseudo_bulk and input bulk data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pseudo_bulk : anndata.AnnData\n",
    "        An :class:`~anndata.AnnData` containing the pseudo expression.\n",
    "    bulk_adata : anndata.AnnData\n",
    "        An :class:`~anndata.AnnData` containing the input expression.\n",
    "    out_dir : string, optional\n",
    "        The path to save the output file.\n",
    "    project : string, optional\n",
    "        The prefix of output file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Returns the expression after removing batch effect.\n",
    "\n",
    "    \"\"\"\n",
    "    out_dir = check_paths(out_dir+'/batch_effect')\n",
    "    if batch_effect:\n",
    "        if exists(f'{out_dir}/{project}_batch_effected.txt'):\n",
    "            print(f'{out_dir}/{project}_batch_effected.txt already exists, skipping batch effect.')\n",
    "            bulk_data = pd.read_csv(f\"{out_dir}/{project}_batch_effected.txt\",sep='\\t').T\n",
    "        else:\n",
    "            \n",
    "            save=True\n",
    "            # check path, file will be stored in out_dir+'/batch_effect'\n",
    "            pseudo_bulk_df = get_count_data(pseudo_bulk)\n",
    "            input_bulk_df= get_count_data(bulk_adata)\n",
    "\n",
    "            bulk = pd.concat([pseudo_bulk_df,input_bulk_df], axis=1)\n",
    "\n",
    "            cells = np.append(pseudo_bulk.obs_names, bulk_adata.obs_names)\n",
    "            batch = np.append(np.ones((1,len(pseudo_bulk.obs_names))), np.ones((1,len(bulk_adata.obs_names)))+1)\n",
    "            if save:\n",
    "                bulk.to_csv(out_dir+f\"/{project}_before_batch_effected.txt\",sep='\\t')\n",
    "            meta = pd.DataFrame({\"batch\": batch,\"cells\":cells})\n",
    "            # get r script path\n",
    "            robjects.r.source('./combat.R')\n",
    "            pandas2ri.activate()\n",
    "            robjects.r.run_combat(bulk, meta, out_dir, project)\n",
    "            # stop auto trans from pandas to r dataframe\n",
    "            pandas2ri.deactivate()\n",
    "            # add layer\n",
    "            if exists(f'{out_dir}/{project}_batch_effected.txt'):\n",
    "                bulk_data = pd.read_csv(f\"{out_dir}/{project}_batch_effected.txt\",sep='\\t').T\n",
    "                print(bulk_data.shape)\n",
    "            else:\n",
    "                raise ValueError('The batch_effected data is not available')\n",
    "        bulk_data.clip(lower=0,inplace=True)\n",
    "        # print(pseudo_bulk)\n",
    "        # print(pseudo_bulk.obs_names)\n",
    "        pseudo_bulk.layers[\"batch_effected\"] = bulk_data.loc[pseudo_bulk.obs_names,:].values\n",
    "        bulk_adata.layers[\"batch_effected\"] = bulk_data.loc[bulk_adata.obs_names,:].values\n",
    "    else:\n",
    "        pseudo_bulk_df = get_count_data(pseudo_bulk)\n",
    "        input_bulk_df= get_count_data(bulk_adata)\n",
    "        bulk = pd.concat([pseudo_bulk_df,input_bulk_df], axis=1)\n",
    "        bulk.to_csv(out_dir+f\"/{project}_batch_effected.txt\",sep='\\t')\n",
    "\n",
    "    return pseudo_bulk,bulk_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_path1 = './datas/23/2/BRCA_GSE110686.h5ad'\n",
    "folder_path1 = './datas/23/2/'\n",
    "radius1 = 0\n",
    "c1 =0\n",
    "epoches1 =20\n",
    "cell_path2 = \"./datas/23/3/CRC_GSE146771.h5ad\" \n",
    "folder_path2 = \"./datas/23/3/\" \n",
    "radius2 = 0\n",
    "c2 =0\n",
    "epoches2 = 20\n",
    "contin = True\n",
    "resolution=1\n",
    "method='average'\n",
    "alignment=1\n",
    "n_pca=50\n",
    "meta_col = 'Celltype..major.lineage.'\n",
    "ms = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_sc.py -cp1 './datas/120/3/sample1_small.h5' -f1 \"./datas/120/3/\" -r1 25 -c1 0.1 -e1 10 -cp2 './datas/120/4/sample1_small.h5' -f2 \"./datas/120/4/\" -r2 25 -c2 0.1 -e2 10 --contin False --alignment 1 --resolution 1 --n_pca 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/orig.ident' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/nCount_RNA' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/nFeature_RNA' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/UMAP_1' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/UMAP_2' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype..malignancy.' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype..major.lineage.' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype..minor.lineage.' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Cluster' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Patient' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Source' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Gender' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Stage' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/_index' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:238: OldFormatWarning: Element '/obsm' was written without encoding metadata.\n",
      "  d[k] = read_elem(f[k])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/var/features' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/var/_index' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:238: OldFormatWarning: Element '/varm' was written without encoding metadata.\n",
      "  d[k] = read_elem(f[k])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:272: OldFormatWarning: Element '/raw/var' was written without encoding metadata.\n",
      "  raw[v] = read_elem(f[f\"raw/{v}\"])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:92: OldFormatWarning: Element '/raw/var/_index' was written without encoding metadata.\n",
      "  return {k: read_elem(v) for k, v in elem.items()}\n",
      "python(46078) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "100%|██████████████████████████████████| 6035/6035 [00:00<00:00, 4431481.90it/s]\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/core.py:136: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  adata = sc.AnnData(ann);\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_core/anndata.py:121: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell meta score for dataset1: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/orig.ident' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/nCount_RNA' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/nFeature_RNA' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/UMAP_1' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/UMAP_2' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype..malignancy.' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype..major.lineage.' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Celltype_minor' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Cluster' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Patient' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Sample' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Source' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Gender' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/Stage' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/TNMstage' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/obs/_index' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:238: OldFormatWarning: Element '/obsm' was written without encoding metadata.\n",
      "  d[k] = read_elem(f[k])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/var/features' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:590: OldFormatWarning: Element '/var/_index' was written without encoding metadata.\n",
      "  return read_elem(dataset)\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:238: OldFormatWarning: Element '/varm' was written without encoding metadata.\n",
      "  d[k] = read_elem(f[k])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py:272: OldFormatWarning: Element '/raw/var' was written without encoding metadata.\n",
      "  raw[v] = read_elem(f[f\"raw/{v}\"])\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/specs/methods.py:92: OldFormatWarning: Element '/raw/var/_index' was written without encoding metadata.\n",
      "  return {k: read_elem(v) for k, v in elem.items()}\n",
      "100%|████████████████████████████████| 10468/10468 [00:00<00:00, 9685853.58it/s]\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/core.py:136: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  adata = sc.AnnData(ann);\n",
      "/Users/dongjiajie/opt/anaconda3/lib/python3.8/site-packages/anndata/_core/anndata.py:121: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell meta score for dataset2: 1.0\n"
     ]
    }
   ],
   "source": [
    "if (contin==False) or ((os.path.exists(folder_path1+'merge_cell_data.csv') and os.path.exists(folder_path1 + 'merge_cell_meta.csv')) == False):\n",
    "    loss1 = merge_by_radius(cell_path1,folder_path1,radius1,method,meta_col)\n",
    "    print(\"cell meta score for dataset1: {}\\n\".format(loss1))\n",
    "else:\n",
    "    print(\"dataset1 find files and skip merging\")\n",
    "\n",
    "\n",
    "\n",
    "if(contin==False) or ((os.path.exists(folder_path2+'merge_cell_data.csv') and os.path.exists(folder_path2 + 'merge_cell_meta.csv')) == False):\n",
    "    loss2 = merge_by_radius(cell_path2,folder_path2,radius2,method,meta_col)\n",
    "    print(\"cell meta score for dataset2: {}\".format(loss2))\n",
    "else:\n",
    "    print(\"dataset2 find files and skip merging\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 15:35:06.464953: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "adata1 = sc.read_h5ad(folder_path1+ 'adata.h5ad')\n",
    "adata2 = sc.read_h5ad(folder_path2+ 'adata.h5ad')\n",
    "\n",
    "\n",
    "preprocessing_cluster(adata1,N_pcs=n_pca,resolution=resolution)\n",
    "preprocessing_cluster(adata2,N_pcs=n_pca,resolution=resolution)\n",
    "\n",
    "inter_gene = sort_data(adata1,adata2)\n",
    "\n",
    "tmp1 = calculate_cluster_centroid_for_genes(adata1,inter_gene,folder_path1) \n",
    "tmp2 = calculate_cluster_centroid_for_genes(adata2,inter_gene,folder_path2)\n",
    "\n",
    "\n",
    "meta_list1 = calculate_cluster_celltype(adata1);\n",
    "meta_list2 = calculate_cluster_celltype(adata2);\n",
    "calculate_meta_ori(folder_path1,adata1);\n",
    "calculate_meta_ori(folder_path2,adata2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datas/23/2/datas.data length:10\n",
      "Generating all pairs superset\n",
      "\t Epoch 0 | average train loss: 0.966356\n",
      "Optimization finished.\n",
      "\t Epoch 1 | average train loss: 0.941207\n",
      "Optimization finished.\n",
      "\t Epoch 2 | average train loss: 0.941279\n",
      "Optimization finished.\n",
      "\t Epoch 3 | average train loss: 0.940414\n",
      "Optimization finished.\n",
      "\t Epoch 4 | average train loss: 0.940350\n",
      "Optimization finished.\n",
      "\t Epoch 5 | average train loss: 0.940468\n",
      "Optimization finished.\n",
      "\t Epoch 6 | average train loss: 0.941214\n",
      "Optimization finished.\n",
      "\t Epoch 7 | average train loss: 0.940824\n",
      "Optimization finished.\n",
      "\t Epoch 8 | average train loss: 0.940168\n",
      "Optimization finished.\n",
      "\t Epoch 9 | average train loss: 0.939446\n",
      "Optimization finished.\n",
      "\t Epoch 10 | average train loss: 0.939698\n",
      "Optimization finished.\n",
      "\t Epoch 11 | average train loss: 0.938994\n",
      "Optimization finished.\n",
      "\t Epoch 12 | average train loss: 0.940198\n",
      "Optimization finished.\n",
      "\t Epoch 13 | average train loss: 0.940589\n",
      "Optimization finished.\n",
      "\t Epoch 14 | average train loss: 0.939682\n",
      "Optimization finished.\n",
      "\t Epoch 15 | average train loss: 0.939953\n",
      "Optimization finished.\n",
      "\t Epoch 16 | average train loss: 0.941084\n",
      "Optimization finished.\n",
      "\t Epoch 17 | average train loss: 0.940519\n",
      "Optimization finished.\n",
      "\t Epoch 18 | average train loss: 0.940264\n",
      "Optimization finished.\n",
      "\t Epoch 19 | average train loss: 0.939095\n",
      "Optimization finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/utils/mst.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ij = np.empty((n - 1, 2), dtype=np.int)\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/utils/mst.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  merged = np.zeros(n, dtype=np.int)\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/utils/mst.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  j = np.empty(n, dtype=np.int)\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/datasets/balance_dataset.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  temp = np.array(temp)\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/model/balancehc.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embeddings.weight.data = torch.tensor(embeddings);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 0 | average train loss: 0.429346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/model/balancehc.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  origin_distance = hyp_dist(torch.tensor(self.original[i]),torch.tensor(self.original[j]));\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/model/balancehc.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  origin_distance = hyp_dist(torch.tensor(self.original[int(nodef.son[0])]),torch.tensor(self.original[k])) + hyp_dist(torch.tensor(self.original[int(nodef.son[1])]),torch.tensor(self.original[k]));\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/model/balancehc.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  total = torch.sum(sims, dim=-1, keepdim=True) - w_ord + self.hyperparamter*torch.abs(origin_distance - hyp_dist(torch.tensor(self.original[int(nodef.son[0])]),e3) - hyp_dist(torch.tensor(self.original[int(nodef.son[1])]),e3));\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 1 | average train loss: 0.423453\n",
      "\t Epoch 2 | average train loss: 0.422780\n",
      "\t Epoch 3 | average train loss: 0.422216\n",
      "\t Epoch 4 | average train loss: 0.421836\n",
      "\t Epoch 5 | average train loss: 0.421476\n",
      "\t Epoch 6 | average train loss: 0.420833\n",
      "\t Epoch 7 | average train loss: 0.419985\n",
      "\t Epoch 8 | average train loss: 0.419198\n",
      "\t Epoch 9 | average train loss: 0.419298\n",
      "./datas/23/3/datas.data length:19\n",
      "Generating all pairs superset\n",
      "\t Epoch 0 | average train loss: 0.975638\n",
      "Optimization finished.\n",
      "\t Epoch 1 | average train loss: 0.969955\n",
      "Optimization finished.\n",
      "\t Epoch 2 | average train loss: 0.970281\n",
      "Optimization finished.\n",
      "\t Epoch 3 | average train loss: 0.969464\n",
      "Optimization finished.\n",
      "\t Epoch 4 | average train loss: 0.970345\n",
      "Optimization finished.\n",
      "\t Epoch 5 | average train loss: 0.969469\n",
      "Optimization finished.\n",
      "\t Epoch 6 | average train loss: 0.969194\n",
      "Optimization finished.\n",
      "\t Epoch 7 | average train loss: 0.968782\n",
      "Optimization finished.\n",
      "\t Epoch 8 | average train loss: 0.968777\n",
      "Optimization finished.\n",
      "\t Epoch 9 | average train loss: 0.969014\n",
      "Optimization finished.\n",
      "\t Epoch 10 | average train loss: 0.969068\n",
      "Optimization finished.\n",
      "\t Epoch 11 | average train loss: 0.968853\n",
      "Optimization finished.\n",
      "\t Epoch 12 | average train loss: 0.968551\n",
      "Optimization finished.\n",
      "\t Epoch 13 | average train loss: 0.968562\n",
      "Optimization finished.\n",
      "\t Epoch 14 | average train loss: 0.968594\n",
      "Optimization finished.\n",
      "\t Epoch 15 | average train loss: 0.968481\n",
      "Optimization finished.\n",
      "\t Epoch 16 | average train loss: 0.968356\n",
      "Optimization finished.\n",
      "\t Epoch 17 | average train loss: 0.968334\n",
      "Optimization finished.\n",
      "\t Epoch 18 | average train loss: 0.968551\n",
      "Optimization finished.\n",
      "\t Epoch 19 | average train loss: 0.968716\n",
      "Optimization finished.\n",
      "\t Epoch 0 | average train loss: 0.466732\n",
      "\t Epoch 1 | average train loss: 0.467281\n",
      "\t Epoch 2 | average train loss: 0.465550\n",
      "\t Epoch 3 | average train loss: 0.463747\n",
      "\t Epoch 4 | average train loss: 0.462516\n",
      "\t Epoch 5 | average train loss: 0.463868\n",
      "\t Epoch 6 | average train loss: 0.464441\n",
      "\t Epoch 7 | average train loss: 0.465502\n",
      "\t Epoch 8 | average train loss: 0.465164\n",
      "\t Epoch 9 | average train loss: 0.466538\n"
     ]
    }
   ],
   "source": [
    "if(contin==False) or ((os.path.exists(folder_path1 + 'dataxy.npy') and os.path.exists(folder_path1+'data1link.npy') and os.path.exists(folder_path1+'dataname.npy')) == False):\n",
    "    get_Hyper_tree(folder_path1+'datas.data',1,tmp1.shape[1]+1,0,epoches1,10,save_path=folder_path1,mst=ms)\n",
    "else:\n",
    "    print(\"dataset1 find files and skip embedding\");\n",
    "\n",
    "if(contin==False) or ((os.path.exists(folder_path2 + 'dataxy.npy') and os.path.exists(folder_path2+'data1link.npy') and os.path.exists(folder_path1+'dataname.npy')) == False):\n",
    "    get_Hyper_tree(folder_path2+'datas.data',1,tmp2.shape[1]+1,0,epoches2,10,save_path=folder_path2,mst=ms)\n",
    "else:\n",
    "    print(\"dataset2 find files and skip embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/alignment.py:749: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  father_name = father_name.astype(np.int)\n",
      "/Users/dongjiajie/Desktop/alignment/fyp/hyperalignment/final/alignment.py:749: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  father_name = father_name.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mininum cost for alignment is 35.747803121248495\n",
      "The alignment edges list is [('18', '36'), ('17', '34'), ('16', '33'), ('14', '32'), ('12', '28'), ('8_CD4Tconv', '11_NK'), ('3_CD8Tex', '13_Malignant'), ('1_CD4Tconv', '25'), ('10', '22'), ('6_CD8T', '5_Treg'), ('0_CD4Tconv', '14_Mast'), ('15', '20'), ('5_CD4Tconv', '7_Plasma'), ('7_CD4Tconv', '1_NK'), ('13', '35'), ('9_Mono/Macro', '3_Malignant'), ('4_Treg', '30')]\n",
      "average cost for one node:0.6383536271651516\n",
      "\n",
      "correct alignment rate:0.0\n"
     ]
    }
   ],
   "source": [
    "nodes1 = build_hyper_tree_from_folder(folder_path1,ms)\n",
    "nodes2 = build_hyper_tree_from_folder(folder_path2,ms)\n",
    "\n",
    "# nodes1[0] = search_tree(nodes1[0],0.5,)\n",
    "# nodes2[0] = search_tree(nodes2[0],15)\n",
    "search_tree\n",
    "def add_meta(now,meta_list,merge):\n",
    "    if(int(now.name)<len(meta_list)):\n",
    "        now.name= now.name +'_'+ meta_list[int(now.name)];\n",
    "    merge.append(now)\n",
    "    for i in now.son:\n",
    "        add_meta(i,meta_list,merge)\n",
    "nodes_merge1 = [];\n",
    "nodes_merge2 = [];\n",
    "add_meta(nodes1[0],meta_list1,nodes_merge1)\n",
    "add_meta(nodes2[0],meta_list2,nodes_merge2)\n",
    "\n",
    "        \n",
    "rate = 0;        \n",
    "if(alignment==1):\n",
    "    rate,anslist,ans = run_alignment(nodes_merge1,nodes_merge2,folder_path1,folder_path2,meta_list1,meta_list2);\n",
    "elif(alignment==2):\n",
    "    rate = run_alignment_linear(nodes_merge1,nodes_merge2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(nodes1)+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(nodes2)+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 13,\n",
       " 4_Treg,\n",
       " 9_Mono/Macro,\n",
       " 17,\n",
       " 15,\n",
       " 7_CD4Tconv,\n",
       " 11,\n",
       " 2_CD8T,\n",
       " 5_CD4Tconv,\n",
       " 16,\n",
       " 10,\n",
       " 0_CD4Tconv,\n",
       " 6_CD8T,\n",
       " 14,\n",
       " 1_CD4Tconv,\n",
       " 12,\n",
       " 3_CD8Tex,\n",
       " 8_CD4Tconv]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(nodes,index):\n",
    "    for i in nodes:\n",
    "        if(int(i.name.split('_')[0]) ==index):\n",
    "            return i;\n",
    "ans = 0;\n",
    "for i1,j1 in anslist:\n",
    "    i =find_index(nodes1,i1);\n",
    "    j = find_index(nodes2,j1)\n",
    "    df = pd.DataFrame(\n",
    "        {\"A\": i.value, \"B\":j.value})\n",
    "    mincost = 1 - df.corr(method=\"spearman\").iloc[0, 1]\n",
    "    ans+=mincost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.574741896758702"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10+19 - 2*len(anslist) + ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df = pd.DataFrame(\n",
    "        {\"A\": i.value, \"B\":j.value})\n",
    "    mincost = 1 - df.corr(method=\"spearman\").iloc[0, 1]\n",
    "    return mincost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 11), (3, 13), (6, 5), (0, 14), (5, 7), (7, 1), (9, 3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anslist_dist = dict(ans)\n",
    "anslist_dist.keys()\n",
    "def search_lineage(now,path,anss):\n",
    "    path.append(now.name)\n",
    "    if(now.son==[]):\n",
    "        anss.append(path);\n",
    "        return\n",
    "    \n",
    "    for i in now.son:\n",
    "        search_lineage(i,path.copy(),anss);\n",
    "temp1 = []\n",
    "search_lineage(nodes1[0],[],temp1)\n",
    "temp1\n",
    "route1 = []\n",
    "route2 = []\n",
    "\n",
    "for i in temp1:\n",
    "    r1 = []\n",
    "    r2 = []\n",
    "    for j in i:\n",
    "        if j in anslist_dist.keys():\n",
    "            r1.append(j)\n",
    "            r2.append(anslist_dist[j])\n",
    "    route1.append(r1)\n",
    "    route2.append(r2)\n",
    "adata1.obs.index = [i+'_1' for i in adata1.obs.index]\n",
    "adata2.obs.index = [i+'_2' for i in adata2.obs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [];\n",
    "\n",
    "\n",
    "for i,j in zip(route1,route2):\n",
    "    try:\n",
    "        shutil.rmtree('./batch_effect/', ignore_errors=True)\n",
    "\n",
    "        cells1 = [ ]\n",
    "        cells2 = [ ]\n",
    "        for k,t in zip(i,j):\n",
    "            num1 = int(k.split('_')[0])\n",
    "            num2 = int(t.split('_')[0])\n",
    "            if(len(k.split('_'))>1):\n",
    "                cells1.append(str(num1));\n",
    "            if(len(t.split('_'))>1):\n",
    "                cells2.append(str(num2));\n",
    "        if(cells1==[] or cells2 ==[]):\n",
    "            continue;\n",
    "        \n",
    "        sub_adata1 = adata1[adata1.obs['leiden'].isin(cells1),inter_gene].copy();\n",
    "        sub_adata2 = adata2[adata2.obs['leiden'].isin(cells2),inter_gene].copy();\n",
    "\n",
    "        adata1_after,adata2_after = remove_batch_effect(sub_adata1.copy(),sub_adata2.copy(),'./')\n",
    "        \n",
    "        sc.pp.neighbors(sub_adata1,use_rep='X')\n",
    "        sc.tl.diffmap(sub_adata1)\n",
    "        sub_adata1.uns['iroot'] = 0\n",
    "        sc.tl.dpt(sub_adata1)\n",
    "        \n",
    "        \n",
    "        sc.pp.neighbors(sub_adata2,use_rep='X')\n",
    "        sc.tl.diffmap(sub_adata2)\n",
    "        sub_adata2.uns['iroot'] = 0\n",
    "        sc.tl.dpt(sub_adata2)\n",
    "        \n",
    "        \n",
    "        adata1_after.obsm['batch_effected'] = adata1_after.layers['batch_effected']\n",
    "        adata2_after.obsm['batch_effected'] = adata2_after.layers['batch_effected']\n",
    "        \n",
    "        sc.pp.neighbors(adata1_after,use_rep='batch_effected')\n",
    "        sc.tl.diffmap(adata1_after)\n",
    "        adata1_after.uns['iroot'] = 0\n",
    "        sc.tl.dpt(adata1_after)\n",
    "\n",
    "        sc.pp.neighbors(adata2_after,use_rep='batch_effected')\n",
    "        sc.tl.diffmap(adata2_after)\n",
    "        adata2_after.uns['iroot'] = 0\n",
    "        sc.tl.dpt(adata2_after)\n",
    "        \n",
    "        score.append( scib.me.trajectory_conservation(sub_adata1, adata1_after, label_key=\"celltype\"))\n",
    "        score.append( scib.me.trajectory_conservation(sub_adata2, adata2_after, label_key=\"celltype\"))\n",
    "        shutil.rmtree('./batch_effect/', ignore_errors=True)\n",
    "    except:\n",
    "        print(cells1,cells2)\n",
    "        pass;\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(score).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.761302636083028"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
